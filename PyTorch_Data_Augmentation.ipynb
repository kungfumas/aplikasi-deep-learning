{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kungfumas/aplikasi-deep-learning/blob/master/PyTorch_Data_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sNMnHHhAkuAa"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation for Computer Vision with PyTorch (Part 1: Image Classification)\n",
        "\n",
        "Let's install PyTorch, and update PIL."
      ]
    },
    {
      "metadata": {
        "id": "esBeTWO4ytGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71594e38-2b5f-4648-de1b-2bd09f3eccc1"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install pillow\n",
        "\n",
        "# Restart Kernel\n",
        "# This workaround is needed to properly upgrade PIL on Google Colab.\n",
        "import os\n",
        "os._exit(00)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchvision) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (9.3.0)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sj6LnGprKH1R"
      },
      "cell_type": "markdown",
      "source": [
        "Next, import all libraries and configure matplotlib to display larger plots."
      ]
    },
    {
      "metadata": {
        "id": "9NRlYXKQy3Kx"
      },
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6o2H9qAjkZg"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['image.interpolation'] = 'nearest'\n",
        "mpl.rcParams['figure.figsize'] = 15, 25\n",
        "\n",
        "def show_dataset(dataset, n=6):\n",
        "  img = np.vstack((np.hstack((np.asarray(dataset[i][0]) for _ in range(n)))\n",
        "                   for i in range(len(dataset))))\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8wcV3p2k4eO"
      },
      "cell_type": "markdown",
      "source": [
        "We need some sample images to perform augmentation. Let's import them from the pytorch-examples repository."
      ]
    },
    {
      "metadata": {
        "id": "fITwt4tOAgBQ"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fabioperez/pytorch-examples/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNvUCPoxLdcH"
      },
      "cell_type": "markdown",
      "source": [
        "Augmenting data with PyTorch is very straightforward. We can use the transforms provided in torchvision: [`torchvision.transforms`](https://pytorch.org/docs/stable/torchvision/transforms.html).\n",
        "\n",
        "To compose several transforms together, we use [`torchvision.transforms.Compose`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose) and pass the transforms as a list. The transforms are applied following the list order.\n",
        "\n",
        "**Important note**\n",
        "\n",
        "For training we should probably also add [`transforms.ToTensor`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor) to convert the images to a PyTorch Tensor and [`transforms.Normalize`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize) to normalize the images according to the network that you will train. I'm omitting these steps since my focus for this tutorial is to display the augmented images.\n"
      ]
    },
    {
      "metadata": {
        "id": "yLEwF_2RzGs0"
      },
      "cell_type": "code",
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKTyAtAiMxYI"
      },
      "cell_type": "markdown",
      "source": [
        "We then pass the transforms to [`torchvision.datasets.ImageFolder`](https://pytorch.org/docs/master/torchvision/datasets.html#imagefolder) and the images will be automatically augmented.\n",
        "\n",
        "If you want to define your own [dataset](https://pytorch.org/docs/master/torchvision/datasets.html) class, you just have to call `transforms(samples)` to perform the transforms on your samples. Check the source code for [`ImageFolder`](https://github.com/pytorch/vision/blob/4db0398a2b02aae790013efbc868f2d795eb2ef7/torchvision/datasets/folder.py#L150) for more details."
      ]
    },
    {
      "metadata": {
        "id": "wpWSjR-HBVsO"
      },
      "cell_type": "code",
      "source": [
        "dataset = torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cl-9XL-qNiBi"
      },
      "cell_type": "markdown",
      "source": [
        "That's all! Let's visualize some of the transformed images:"
      ]
    },
    {
      "metadata": {
        "id": "BM7-0e4pzLXN"
      },
      "cell_type": "code",
      "source": [
        "show_dataset(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WDgWayyknCRH"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, but the augmentations available on PyTorch are very simple. What if we want to perform more interesting augmentations?\n",
        "\n",
        "Let's use [imgaug](https://github.com/aleju/imgaug) to achieve that."
      ]
    },
    {
      "metadata": {
        "id": "aFtwoTIS0woY"
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/aleju/imgaug\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7yrsg7oSlN9S"
      },
      "cell_type": "markdown",
      "source": [
        "We can perform more complex augmentation and define more complex behaviors with imgaug.\n",
        "\n",
        "Besides having more image transformation techniques, imgaug also has interesting behaviors such as [`Sometimes`](http://imgaug.readthedocs.io/en/latest/source/augmenters.html#sometimes), [`SomeOf`](http://imgaug.readthedocs.io/en/latest/source/augmenters.html#someof) and [`OneOf`](http://imgaug.readthedocs.io/en/latest/source/augmenters.html#oneof).\n",
        "\n",
        "Note that for imgaug, we should convert the PIL images to NumPy arrays before applying the transforms.\n",
        "\n",
        "You should always be careful with the `dtype` and dimension ordering when applying data augmentation and training neural networks. It's a good practice to visualize the augmented images and print some of the input values before training a neural network."
      ]
    },
    {
      "metadata": {
        "id": "8q8a2Ha9pnaz"
      },
      "cell_type": "code",
      "source": [
        "class ImgAugTransform:\n",
        "  def __init__(self):\n",
        "    self.aug = iaa.Sequential([\n",
        "        iaa.Scale((224, 224)),\n",
        "        iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
        "        iaa.Fliplr(0.5),\n",
        "        iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n",
        "        iaa.Sometimes(0.25,\n",
        "                      iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
        "                                 iaa.CoarseDropout(0.1, size_percent=0.5)])),\n",
        "        iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
        "    ])\n",
        "      \n",
        "  def __call__(self, img):\n",
        "    img = np.array(img)\n",
        "    return self.aug.augment_image(img)\n",
        "\n",
        "transforms = ImgAugTransform()\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8WJoSAh2qVLw"
      },
      "cell_type": "code",
      "source": [
        "show_dataset(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDdkb0CYRKuZ"
      },
      "cell_type": "markdown",
      "source": [
        "imgaug also accepts a list of images instead of a single image. To augment them, use `aug.augment_images`. This may be useful if you are implementing a custom dataset that will load several images at once, before augmenting them. "
      ]
    },
    {
      "metadata": {
        "id": "r8PI3SKZqY8H"
      },
      "cell_type": "code",
      "source": [
        "aug = iaa.Affine(rotate=(-40, 40), mode='symmetric')\n",
        "imgs = [np.asarray(dataset[0][0]) for _ in range(6)]\n",
        "aug.augment_images(imgs)\n",
        "plt.imshow(np.hstack(imgs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SuuB7fXWTEKW"
      },
      "cell_type": "markdown",
      "source": [
        "If you want to create your own image transformations, you just have to write a Python function that will transform the given image (or images) and use it with [`transforms.Lambda`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Lambda), [`iaa.Lambda`](http://imgaug.readthedocs.io/en/latest/source/augmenters.html#lambda), or to call it directly in a custom transform class."
      ]
    },
    {
      "metadata": {
        "id": "DBJWYcRiVknc"
      },
      "cell_type": "markdown",
      "source": [
        "Mixing PyTorch and imgaug transforms is also very simple:"
      ]
    },
    {
      "metadata": {
        "id": "rQ6DFPNvVD8s"
      },
      "cell_type": "code",
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "    ImgAugTransform(),\n",
        "    lambda x: PIL.Image.fromarray(x),\n",
        "    torchvision.transforms.RandomVerticalFlip()\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms)\n",
        "\n",
        "show_dataset(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i4KMLRFpsATw"
      },
      "cell_type": "markdown",
      "source": [
        "# Time\n",
        "\n",
        "Let's perform a simple benchmark to compare the running time for imgaug and for PyTorch. We can see that the results are very similar for the same augmentation techniques."
      ]
    },
    {
      "metadata": {
        "id": "aUpukiy8sBKx"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "transforms_pytorch = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(20)\n",
        "])\n",
        "\n",
        "class ImgAugTransform:\n",
        "  def __init__(self):\n",
        "    self.aug = iaa.Sequential([\n",
        "        iaa.Scale((224, 224)),\n",
        "        iaa.AddToHueAndSaturation(value=(-20, 20), per_channel=True),\n",
        "        iaa.Fliplr(0.5),\n",
        "        iaa.Affine(rotate=(-20, 20), mode='constant'),\n",
        "    ])\n",
        "      \n",
        "  def __call__(self, img):\n",
        "    img = np.array(img)\n",
        "    return self.aug.augment_image(img)\n",
        "\n",
        "transforms_imgaug = ImgAugTransform()\n",
        "\n",
        "datasets = {\n",
        "'pytorch': torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms_pytorch),\n",
        "'imgaug' : torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms_imgaug)\n",
        "}\n",
        "\n",
        "times = {'pytorch': [], 'imgaug': []}\n",
        "for _ in range(20):\n",
        "  for mode in ('pytorch', 'imgaug'):\n",
        "    start = time.time()\n",
        "    img_pytorch = np.vstack((np.hstack((np.asarray(datasets[mode][i][0]) for _ in range(6))) for i in range(4)))\n",
        "    end = time.time()\n",
        "    times[mode].append(end - start)\n",
        "    \n",
        "for mode in ('pytorch', 'imgaug'):\n",
        "  t = np.array(times[mode])\n",
        "  print(\"{}: {:.04f}\".format(mode, t.min()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WzjlXx7HXof2"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}