{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pose classification (basic).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kungfumas/aplikasi-deep-learning/blob/master/Pose_classification_tari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyGX4hRSHU1d"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This Colab helps to create a training set for the k-NN classifier described in the MediaPipe [Pose Classification](https://google.github.io/mediapipe/solutions/pose_classification.html) soultion, export it to a CSV and then use it in the [ML Kit sample app](https://developers.google.com/ml-kit/vision/pose-detection/classifying-poses#4_integrate_with_the_ml_kit_quickstart_app)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DpCfLGxsdzg"
      },
      "source": [
        "# Step 0: Start Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKqTTP3XsekH"
      },
      "source": [
        "Connect the Colab to hosted Python3 runtime (check top-right corner) and then install required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpZrq2Ktsgdu"
      },
      "source": [
        "!pip install numpy==1.19.3\n",
        "!pip install opencv-python==4.5.1.48\n",
        "!pip install tqdm==4.56.0\n",
        "\n",
        "!pip install mediapipe==0.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgHTsKdz7cn_"
      },
      "source": [
        "# Step 1: Upload image samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BfvGuy37ih1"
      },
      "source": [
        "Locally create a folder named `fitness_poses_images_in` with image samples.\n",
        "\n",
        "Images should repesent terminal states of desired pose classes. I.e. if you want to classify push-up provide iamges for two classes: when person is up, and when person is down.\n",
        "\n",
        "There should be about a few hundred samples per class covering different camera angles, environment conditions, body shapes, and exercise variations to build a good classifier.\n",
        "\n",
        "Required structure of the images_in_folder:\n",
        "```\n",
        "fitness_poses_images_in/\n",
        "  pushups_up/\n",
        "    image_001.jpg\n",
        "    image_002.jpg\n",
        "    ...\n",
        "  pushups_down/\n",
        "    image_001.jpg\n",
        "    image_002.jpg\n",
        "    ...\n",
        "  ...\n",
        "```\n",
        "\n",
        "Zip the `fitness_poses_images_in` folder:\n",
        "```\n",
        "zip -r fitness_poses_images_in.zip fitness_poses_images_in\n",
        "```\n",
        "\n",
        "And run the code below to upload it to the Colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJHy8QbK7f9Z"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "os.listdir('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MUhn--d82Mv"
      },
      "source": [
        "Unzip the archive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PvT2NOt863S"
      },
      "source": [
        "import zipfile\n",
        "import io\n",
        "\n",
        "zf = zipfile.ZipFile(io.BytesIO(uploaded['fitness_poses_images_in.zip']), \"r\")\n",
        "zf.extractall()\n",
        "os.listdir('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdtsJ8TvqktY"
      },
      "source": [
        "# Step 2: Create samples for classifier\n",
        "\n",
        "Runs BlazePose on provided images to get target poses for the classifier in a format required by the demo App."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PiV3r_Pphln"
      },
      "source": [
        "# Folder with images to use as target poses for classification.\n",
        "#\n",
        "# Images should repesent terminal states of desired pose classes. I.e. if you\n",
        "# want to classify push-up provide iamges for two classes: when person is up,\n",
        "# and when person is down.\n",
        "#\n",
        "# Required structure of the images_in_folder:\n",
        "#   fitness_poses_images_in/\n",
        "#     pushups_up/\n",
        "#       image_001.jpg\n",
        "#       image_002.jpg\n",
        "#       ...\n",
        "#     pushups_down/\n",
        "#       image_001.jpg\n",
        "#       image_002.jpg\n",
        "#       ...\n",
        "#     ...\n",
        "images_in_folder = 'fitness_poses_images_in'\n",
        "\n",
        "# Output folders for bootstrapped images and CSVs. Image will have a predicted\n",
        "# Pose rendering and can be used to remove unwanted samples.\n",
        "images_out_folder = 'fitness_poses_images_out_basic'\n",
        "\n",
        "# Output CSV path to put bootstrapped poses to. This CSV will be used by the\n",
        "# demo App.\n",
        "#\n",
        "# Output CSV format:\n",
        "#   sample_00001,pose_class_1,x1,y1,z1,x2,y2,z2,...,x33,y33,z33\n",
        "#   sample_00002,pose_class_2,x1,y1,z1,x2,y2,z2,...,x33,y33,z33\n",
        "#   ...\n",
        "#\n",
        "csv_out_path = 'fitness_poses_csvs_out_basic.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btboitEDrSDq"
      },
      "source": [
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tqdm\n",
        "\n",
        "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
        "from mediapipe.python.solutions import pose as mp_pose\n",
        "\n",
        "\n",
        "with open(csv_out_path, 'w') as csv_out_file:\n",
        "  csv_out_writer = csv.writer(csv_out_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "  # Folder names are used as pose class names.\n",
        "  pose_class_names = sorted([n for n in os.listdir(images_in_folder) if not n.startswith('.')])\n",
        "\n",
        "  for pose_class_name in pose_class_names:\n",
        "    print('Bootstrapping ', pose_class_name, file=sys.stderr)\n",
        "\n",
        "    if not os.path.exists(os.path.join(images_out_folder, pose_class_name)):\n",
        "      os.makedirs(os.path.join(images_out_folder, pose_class_name))\n",
        "\n",
        "    image_names = sorted([\n",
        "        n for n in os.listdir(os.path.join(images_in_folder, pose_class_name))\n",
        "        if not n.startswith('.')])\n",
        "    for image_name in tqdm.tqdm(image_names, position=0):\n",
        "      # Load image.\n",
        "      input_frame = cv2.imread(os.path.join(images_in_folder, pose_class_name, image_name))\n",
        "      input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      # Initialize fresh pose tracker and run it.\n",
        "      with mp_pose.Pose(upper_body_only=False) as pose_tracker:\n",
        "        result = pose_tracker.process(image=input_frame)\n",
        "        pose_landmarks = result.pose_landmarks\n",
        "      \n",
        "      # Save image with pose prediction (if pose was detected).\n",
        "      output_frame = input_frame.copy()\n",
        "      if pose_landmarks is not None:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image=output_frame,\n",
        "            landmark_list=pose_landmarks,\n",
        "            connections=mp_pose.POSE_CONNECTIONS)\n",
        "      output_frame = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
        "      cv2.imwrite(os.path.join(images_out_folder, image_name), output_frame)\n",
        "      \n",
        "      # Save landmarks.\n",
        "      if pose_landmarks is not None:\n",
        "        # Check the number of landmarks and take pose landmarks.\n",
        "        assert len(pose_landmarks.landmark) == 33, 'Unexpected number of predicted pose landmarks: {}'.format(len(pose_landmarks.landmark))\n",
        "        pose_landmarks = [[lmk.x, lmk.y, lmk.z] for lmk in pose_landmarks.landmark]\n",
        "\n",
        "        # Map pose landmarks from [0, 1] range to absolute coordinates to get\n",
        "        # correct aspect ratio.\n",
        "        frame_height, frame_width = output_frame.shape[:2]\n",
        "        pose_landmarks *= np.array([frame_width, frame_height, frame_width])\n",
        "\n",
        "        # Write pose sample to CSV.\n",
        "        pose_landmarks = np.around(pose_landmarks, 5).flatten().astype(np.str).tolist()\n",
        "        csv_out_writer.writerow([image_name, pose_class_name] + pose_landmarks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfD86A2d1-8j"
      },
      "source": [
        "Now look at the output images with predicted Pose and remove those you are not satisfied with from the output CSV. Wrongly predicted poses will affect accuracy of the classification.\n",
        "\n",
        "Once done, you can use the CSV in the demo App.\n",
        "\n",
        "For more accurate validation of the predicted Poses use extended Colab provided in the documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIMSXunH9g6P"
      },
      "source": [
        "# Step 3: Download CSV\n",
        "\n",
        "Please check this [guide](https://developers.google.com/ml-kit/vision/pose-detection/classifying-poses#4_integrate_with_the_ml_kit_quickstart_app) on how to use this CSV in the ML Kit sample app."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90E4HOoJ9uMY"
      },
      "source": [
        "files.download(csv_out_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}